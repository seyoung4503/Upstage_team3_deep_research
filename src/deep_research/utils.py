
"""Research Utilities and Tools.

This module provides search and content processing utilities for the research agent,
including web search capabilities and content summarization tools.
"""

from pathlib import Path
from datetime import datetime
from typing_extensions import Annotated, List, Literal
import os
import re

from langchain_upstage import ChatUpstage
from dotenv import load_dotenv
load_dotenv()

from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool, InjectedToolArg
from tavily import TavilyClient

from deep_research.state_research import Summary
from deep_research.prompts import summarize_webpage_prompt


from pydantic import BaseModel
from typing import List, Literal, Annotated, Optional

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

import requests
from typing import Annotated, Optional, Dict



# ===== UTILITY FUNCTIONS =====

def get_today_str() -> str:
    """Get current date in a human-readable format."""
    return datetime.now().strftime("%a %b %-d, %Y")

def get_current_dir() -> Path:
    """Get the current directory of the module.

    This function is compatible with Jupyter notebooks and regular Python scripts.

    Returns:
        Path object representing the current directory
    """
    try:
        return Path(__file__).resolve().parent
    except NameError:  # __file__ is not defined
        return Path.cwd()

# ===== CONFIGURATION =====

# summarization_model = ChatGoogleGenerativeAI(
#     model="gemini-2.5-flash", 
#     api_key = API_KEY,
#     temperature=0,
#     convert_system_message_to_human=True 
# )

summarization_model = ChatUpstage(api_key=os.getenv("UPSTAGE_API_KEY"), model="solar-pro2", temperature=0)

tavily_client = TavilyClient()

# ===== SEARCH FUNCTIONS =====

def tavily_search_multiple(
    search_queries: List[str], 
    max_results: int = 3, 
    topic: Literal["general", "news", "finance"] = "general", 
    include_raw_content: bool = True, 
) -> List[dict]:
    """Perform search using Tavily API for multiple queries.

    Args:
        search_queries: List of search queries to execute
        max_results: Maximum number of results per query
        topic: Topic filter for search results
        include_raw_content: Whether to include raw webpage content

    Returns:
        List of search result dictionaries
    """

    # Execute searches sequentially. Note: yon can use AsyncTavilyClient to parallelize this step.
    search_docs = []
    for query in search_queries:
        result = tavily_client.search(
            query,
            max_results=max_results,
            include_raw_content=include_raw_content,
            topic=topic
        )
        search_docs.append(result)

    return search_docs

# todo : ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•„ìš”
def summarize_webpage_content(webpage_content: str) -> str: 
    """Summarize webpage content using the configured summarization model.

    Args:
        webpage_content: Raw webpage content to summarize

    Returns:
        Formatted summary with key excerpts
    """
    MAX_CHARS = 15000  
    try:
        truncated = webpage_content[:MAX_CHARS]

        structured_model = summarization_model.with_structured_output(Summary)
        summary = structured_model.invoke([
            HumanMessage(content=summarize_webpage_prompt.format(
                webpage_content=truncated,
                date=get_today_str()
            ))
        ])
        formatted_summary = (
            f"<summary>\n{summary.summary}\n</summary>\n\n"
            f"<key_excerpts>\n{summary.key_excerpts}\n</key_excerpts>"
        )
        return formatted_summary
    except Exception as e:
        print(f"Failed to summarize webpage: {str(e)}")
        return webpage_content[:2000] + "..."
    # try:
    #     # Set up structured output model for summarization
    #     structured_model = summarization_model.with_structured_output(Summary)

    #     # Generate summary
    #     summary = structured_model.invoke([
    #         HumanMessage(content=summarize_webpage_prompt.format(
    #             webpage_content=webpage_content, 
    #             date=get_today_str()
    #         ))
    #     ])

    #     # Format summary with clear structure
    #     formatted_summary = (
    #         f"<summary>\n{summary.summary}\n</summary>\n\n"
    #         f"<key_excerpts>\n{summary.key_excerpts}\n</key_excerpts>"
    #     )

    #     return formatted_summary

    # except Exception as e:
    #     print(f"Failed to summarize webpage: {str(e)}")
    #     return webpage_content[:1000] + "..." if len(webpage_content) > 1000 else webpage_content

def deduplicate_search_results(search_results: List[dict]) -> dict:
    """Deduplicate search results by URL to avoid processing duplicate content.

    Args:
        search_results: List of search result dictionaries

    Returns:
        Dictionary mapping URLs to unique results
    """
    unique_results = {}

    for response in search_results:
        for result in response['results']:
            url = result['url']
            if url not in unique_results:
                unique_results[url] = result

    return unique_results

def process_search_results(unique_results: dict) -> dict:
    """Process search results by summarizing content where available.

    Args:
        unique_results: Dictionary of unique search results

    Returns:
        Dictionary of processed results with summaries
    """
    summarized_results = {}

    for url, result in unique_results.items():
        # Use existing content if no raw content for summarization
        if not result.get("raw_content"):
            content = result['content']
        else:
            # Summarize raw content for better processing
            content = summarize_webpage_content(result['raw_content'])

        summarized_results[url] = {
            'title': result['title'],
            'content': content
        }

    return summarized_results

def format_search_output(summarized_results: dict) -> str:
    """Format search results into a well-structured string output.

    Args:
        summarized_results: Dictionary of processed search results

    Returns:
        Formatted string of search results with clear source separation
    """
    if not summarized_results:
        return "No valid search results found. Please try different search queries or use a different search API."

    formatted_output = "Search results: \n\n"

    for i, (url, result) in enumerate(summarized_results.items(), 1):
        formatted_output += f"\n\n--- SOURCE {i}: {result['title']} ---\n"
        formatted_output += f"URL: {url}\n\n"
        formatted_output += f"SUMMARY:\n{result['content']}\n\n"
        formatted_output += "-" * 80 + "\n"

    return formatted_output

# ===== RESEARCH TOOLS =====

@tool(parse_docstring=True)
def tavily_search(
    query: str,
    max_results: Annotated[int, InjectedToolArg] = 3,
    topic: Annotated[Literal["general", "news", "finance"], InjectedToolArg] = "general",
) -> str:
    """Fetch results from Tavily search API with content summarization.

    Args:
        query: A single search query to execute
        max_results: Maximum number of results to return
        topic: Topic to filter results by ('general', 'news', 'finance')

    Returns:
        Formatted string of search results with summaries
    """
    # Execute search for single query
    search_results = tavily_search_multiple(
        [query],  # Convert single query to list for the internal function
        max_results=max_results,
        topic=topic,
        include_raw_content=False, # todo : html check
    )

    # Deduplicate results by URL to avoid processing duplicate content
    unique_results = deduplicate_search_results(search_results)

    # Process results with summarization
    summarized_results = process_search_results(unique_results)

    # Format output for consumption
    return format_search_output(summarized_results)

@tool(parse_docstring=True)
def think_tool(reflection: str) -> str:
    """Tool for strategic reflection on research progress and decision-making.

    Use this tool after each search to analyze results and plan next steps systematically.
    This creates a deliberate pause in the research workflow for quality decision-making.

    When to use:
    - After receiving search results: What key information did I find?
    - Before deciding next steps: Do I have enough to answer comprehensively?
    - When assessing research gaps: What specific information am I still missing?
    - Before concluding research: Can I provide a complete answer now?

    Reflection should address:
    1. Analysis of current findings - What concrete information have I gathered?
    2. Gap assessment - What crucial information is still missing?
    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?
    4. Strategic decision - Should I continue searching or provide my answer?

    Args:
        reflection: Your detailed reflection on research progress, findings, gaps, and next steps

    Returns:
        Confirmation that reflection was recorded for decision-making
    """
    return f"Reflection recorded: {reflection}"




NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ==========================
# 1) Query Refiner êµ¬ì¡°ì²´
# ==========================

class RefinedQuery(BaseModel):
    """Structured output for Naver query refinement."""
    refined_query: str
    # "ìµœì‹ "/"í˜„ì¬"/"ì–´ì œ" ë“± time-sensitive ì—¬ë¶€
    needs_recency: bool = False
    # ì„ íƒ: ì‚¬ì´íŠ¸ íŒíŠ¸ (ë‚˜ë¬´ìœ„í‚¤, ìœ„í‚¤ë°±ê³¼ ë“±)
    site_hint: Optional[str] = None


# ë„¤ê°€ ì“°ë˜ ëª¨ë¸ ê·¸ëŒ€ë¡œ
query_refiner_model = ChatUpstage(model="solar-pro", temperature=0.0)


def refine_kr_search_query(original_question: str) -> RefinedQuery:
    """
    LLMìœ¼ë¡œ ë„¤ì´ë²„ìš© ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì •ì œí•˜ëŠ” **ë™ê¸°** í•¨ìˆ˜.
    RefinedQuery(pydantic) í˜•íƒœë¡œ ë°˜í™˜ëœë‹¤.
    """
    today = get_today_str()

    system_msg = (
        "You are a Korean search query refiner for Naver (ë‰´ìŠ¤/ì›¹/ë¸”ë¡œê·¸) search.\n"
        "Your job is to convert a natural language question into a concise, "
        "search-engine-friendly Korean query.\n"
        "You MUST output a JSON object with fields: refined_query, needs_recency, site_hint."
    )

    user_msg = f"""
    ë„ˆëŠ” 'ë„¤ì´ë²„ ê²€ìƒ‰ ì—”ì§„'ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì™„ë²½íˆ ì´í•´í•˜ëŠ” [ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì „ë¬¸ê°€]ì•¼.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„, ë„¤ì´ë²„ì—ì„œ **ê°€ì¥ ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” 'í‚¤ì›Œë“œ ì¡°í•©'**ìœ¼ë¡œ ë³€í™˜í•´.

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {original_question}

    [ê¸°ì¤€ì¼]: {today}

    [ë³€í™˜ ê·œì¹™]
    1. ë¬¸ì¥ì„ í•´ì²´í•˜ë¼: ì¡°ì‚¬('ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼')ì™€ ì„œìˆ ì–´('ì•Œë ¤ì¤˜', 'ê¶ê¸ˆí•´')ë¥¼ ëª¨ë‘ ì œê±°í•´.
    2. í–‰ì • ìš©ì–´ ê¸ˆì§€: 'í†µê³„', 'í˜„í™©', 'ìˆ˜ì¹˜', 'ê¸°ì¤€' ê°™ì€ ë”±ë”±í•œ ë‹¨ì–´ëŠ” ë¸”ë¡œê·¸/ë‰´ìŠ¤ ì œëª©ì— ì˜ ì•ˆ ì“°ì´ë‹ˆ ë˜ë„ë¡ í”¼í•˜ê³ ,
       ëŒ€ì‹  'ê·¼í™©', 'ìµœì‹ ', 'ì†ë³´', 'ë°œí‘œ', 'ëª‡ê¶Œ', 'ì–¼ë§ˆ', 'ê°€ê²©' ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì¨.
    3. ë‚ ì§œ êµ¬ì²´í™”:
       - ì§ˆë¬¸ì— 'ì–´ì œ', 'ì˜¤ëŠ˜', 'í˜„ì¬', 'ìµœê·¼', 'ì˜¬í•´' ë“±ì´ ë‚˜ì˜¤ë©´ [ê¸°ì¤€ì¼]ì„ ê¸°ì¤€ìœ¼ë¡œ
         'YYYYë…„ Mì›” Dì¼' ë˜ëŠ” 'YYYYë…„' ê°™ì€ êµ¬ì²´ì ì¸ í‘œí˜„ì„ í¬í•¨í•˜ëŠ” ê²€ìƒ‰ì–´ë¡œ ë§Œë“¤ì–´ë¼.
    4. ì„¹ì…˜ íƒ€ê²ŸíŒ…(íŒíŠ¸ ì°¨ì›):
       - ìˆ˜ì¹˜/ê°€ê²© ì§ˆë¬¸ -> 'ê°€ê²©', 'ì–¼ë§ˆ'
       - ì¸ë¬¼ ì§ˆë¬¸ -> 'í”„ë¡œí•„', 'ë‚˜ì´', 'ìµœê·¼'
       - ë§Œí™”/ì±… ì§ˆë¬¸ -> 'ëª‡ê¶Œ', 'ì‹ ê°„', 'ë°œë§¤ì¼'
       - ì»¤ë®¤ë‹ˆí‹°/ë°˜ì‘ -> 'í›„ê¸°', 'ë°˜ì‘'

    [ì¶œë ¥ í˜•ì‹ - ì¤‘ìš”]
    ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•´. ë‹¤ë¥¸ ë¬¸ì¥ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆ.
    {{
      "refined_query": "<ë„¤ì´ë²„ ê²€ìƒ‰ì°½ì— ë„£ì„ ìµœì¢… ê²€ìƒ‰ì–´>",
      "needs_recency": <true ë˜ëŠ” false>,
      "site_hint": "<'ë‚˜ë¬´ìœ„í‚¤', 'ìœ„í‚¤ë°±ê³¼', 'ê³µì‹ ì‚¬ì´íŠ¸', 'ì—†ìŒ' ì¤‘ í•˜ë‚˜ ë˜ëŠ” null>"
    }}

    - refined_query: ë„¤ì´ë²„ ê²€ìƒ‰ì°½ì— ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ì¢‹ì€ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¡°í•©.
    - needs_recency: 'í˜„ì¬/ìµœê·¼/ì–´ì œ/ì˜¬í•´/ì§€ê¸ˆ' ë“± ì‹œê°„ì´ ì¤‘ìš”í•œ ì§ˆë¬¸ì´ë©´ true, ì•„ë‹ˆë©´ false.
    - site_hint: íŠ¹ì • ì‚¬ì´íŠ¸ê°€ ìœ ë¦¬í•˜ë©´ ê°„ë‹¨íˆ íŒíŠ¸. ì—†ìœ¼ë©´ "ì—†ìŒ" ë˜ëŠ” null.
    """

    structured = query_refiner_model.with_structured_output(RefinedQuery)
    result: RefinedQuery = structured.invoke(
        [
            HumanMessage(role="system", content=system_msg),
            HumanMessage(role="user", content=user_msg),
        ]
    )
    return result



# ===================================
# 2) Playwright + BeautifulSoup ë³¸ë¬¸ ì¶”ì¶œ (sync)
# ===================================

def fetch_clean_content(url: str) -> str:
    """
    URLì— ì ‘ì†í•˜ì—¬ 'ë³¸ë¬¸ ì˜ì—­'ë§Œ ìµœëŒ€í•œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” **ë™ê¸° í•¨ìˆ˜**.
    - ë„¤ì´ë²„ ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì¹´í˜, ì¼ë°˜ ì›¹ ë“±ì„ ì²˜ë¦¬
    - ë„ˆë¬´ ì§§ê±°ë‚˜, ë¡œê·¸ì¸ ë§‰íˆë©´ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
    """
    clean_text = ""

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                )
            )
            page = context.new_page()

            try:
                page.goto(url, timeout=5000, wait_until="domcontentloaded")
            except Exception:
                # íƒ€ì„ì•„ì›ƒ ë‚˜ë”ë¼ë„ ì¼ë‹¨ DOM ìˆëŠ” ë²”ìœ„ì—ì„œ ì§„í–‰
                pass

            try:
                page.wait_for_load_state("networkidle", timeout=2000)
            except Exception:
                pass

            target_frame = page
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ iframe ì²˜ë¦¬
            if "blog.naver.com" in url:
                frame = page.frame(name="mainFrame")
                if frame:
                    target_frame = frame

            html = target_frame.content()
            browser.close()

        soup = BeautifulSoup(html, "html.parser")

        # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼/ë„¤ë¹„/í‘¸í„° ì œê±°
        for tag in soup(["script", "style", "header", "footer", "nav", "aside", "form", "iframe"]):
            tag.decompose()

        main_content = None

        if "blog.naver.com" in url:
            main_content = soup.select_one(".se-main-container") or soup.select_one("#postViewArea")
        elif "n.news.naver.com" in url:
            main_content = soup.select_one("#dic_area") or soup.select_one("#articleBodyContents")
        elif "cafe.naver.com" in url:
            main_content = soup.select_one(".gate_box")  # ë¡œê·¸ì¸ ë§‰í˜€ë„ ëŒ€ë¬¸ í…ìŠ¤íŠ¸ ì •ë„
        else:
            main_content = soup.body

        target_soup = main_content if main_content else soup
        clean_text = target_soup.get_text(separator=" ", strip=True)
        clean_text = re.sub(r"\s+", " ", clean_text)

        if "ë¡œê·¸ì¸" in clean_text and "í•´ì£¼ì„¸ìš”" in clean_text:
            return "ğŸ”’ [ì ‘ê·¼ ì œí•œ] ë¡œê·¸ì¸ í•„ìš”í•œ í˜ì´ì§€ì…ë‹ˆë‹¤."

        return clean_text[:4000]  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„

    except Exception as e:
        return f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}"


# ======================================
# 3) Naver OpenAPI + scraping
# ======================================

def deep_search_naver_internal(
    refined_query: str,
    needs_recency: bool,
    max_results: int = 5,
) -> Dict[str, Dict[str, str]]:
    """
    Naver OpenAPI(news/webkr/blog) + Playwrightë¡œ ê²€ìƒ‰ ë° ë³¸ë¬¸ ì¶”ì¶œ (**ë™ê¸°**).
    ë°˜í™˜ê°’: { url: {title, content} } í˜•íƒœ (tavilyì™€ ë§ì¶”ê¸° ìœ„í•´ dict ì‚¬ìš©)
    """
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        return {}

    sections = ["news", "webkr", "blog"]
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }

    sort_opt = "date" if needs_recency else "sim"

    results_by_url: Dict[str, Dict[str, str]] = {}
    per_section = max(1, max_results // len(sections))  # ì„¹ì…˜ë‹¹ ê°œìˆ˜ ì œí•œ

    for section in sections:
        api_url = f"https://openapi.naver.com/v1/search/{section}.json"
        params = {
            "query": refined_query,
            "display": per_section,
            "start": 1,
            "sort": sort_opt,
        }

        try:
            resp = requests.get(api_url, headers=headers, params=params, timeout=5)
            data = resp.json()
            items = data.get("items", [])

            for item in items:
                raw_title = item.get("title", "")
                title = re.sub("<[^<]+?>", "", raw_title)
                link = item.get("link", "")

                if not link or link in results_by_url:
                    continue

                full_text = fetch_clean_content(link)

                if len(full_text) < 50 or "ë¡œê·¸ì¸" in full_text:
                    desc = item.get("description", "")
                    full_text = f"(ìš”ì•½) {re.sub('<[^<]+?>', '', desc)}"

                results_by_url[link] = {
                    "title": title,
                    "content": full_text,
                }

                if len(results_by_url) >= max_results:
                    break

            if len(results_by_url) >= max_results:
                break

        except Exception as e:
            print(f"[Naver API Error] section={section} err={e}")
            continue

        # âœ… asyncê°€ ì•„ë‹ˆë¯€ë¡œ asyncio.sleep ì œê±° (ì›í•˜ë©´ time.sleep ì‚¬ìš© ê°€ëŠ¥)
        # import time; time.sleep(0.3)

    return results_by_url


# ================================
# 4) ìš”ì•½ (Tavily summarizer ì¬í™œìš©)
# ================================

def summarize_naver_results(results_by_url: Dict[str, Dict[str, str]]) -> Dict[str, Dict[str, str]]:
    """
    Tavily íŒŒì´í”„ë¼ì¸ê³¼ ë§ì¶”ê¸° ìœ„í•´:
    - raw contentë¥¼ summarization_modelë¡œ ìš”ì•½
    - { url: {title, content(summary)} } í˜•íƒœë¡œ ë³€í™˜
    """
    summarized: Dict[str, Dict[str, str]] = {}
    MAX_CHARS = 15000

    structured_model = summarization_model.with_structured_output(Summary)

    for url, r in results_by_url.items():
        raw = r["content"]
        truncated = raw[:MAX_CHARS]

        try:
            summary_obj = structured_model.invoke([
                HumanMessage(
                    content=summarize_webpage_prompt.format(
                        webpage_content=truncated,
                        date=get_today_str(),
                    )
                )
            ])
            formatted = (
                f"<summary>\n{summary_obj.summary}\n</summary>\n\n"
                f"<key_excerpts>\n{summary_obj.key_excerpts}\n</key_excerpts>"
            )
        except Exception as e:
            print(f"[Naver summarize error] {e}")
            formatted = raw[:2000] + "..."

        summarized[url] = {
            "title": r["title"],
            "content": formatted,
        }

    return summarized


def format_search_output(summarized_results: Dict[str, Dict[str, str]]) -> str:
    """
    Tavilyìš© format_search_output í¬ë§·ê³¼ ë™ì¼í•˜ê²Œ.
    """
    if not summarized_results:
        return "No valid search results found. Please try different search queries or use a different search API."

    formatted_output = "Search results:\n\n"

    for i, (url, result) in enumerate(summarized_results.items(), 1):
        formatted_output += f"\n\n--- SOURCE {i}: {result['title']} ---\n"
        formatted_output += f"URL: {url}\n\n"
        formatted_output += f"SUMMARY:\n{result['content']}\n\n"
        formatted_output += "-" * 80 + "\n"

    return formatted_output


# ==========================
# 5) ìµœì¢… Tool: naver_search (sync)
# ==========================

def _naver_search_impl(
    question: str,
    max_results: int = 5,
) -> str:
    # 1) ì¿¼ë¦¬ ë¦¬íŒŒì¸
    refined = refine_kr_search_query(question)
    print(f"[NAVER] original: {question}")
    print(f"[NAVER] refined : {refined.refined_query} (needs_recency={refined.needs_recency})")

    # 2) Naver ê²€ìƒ‰ + ìŠ¤í¬ë˜í•‘
    raw_results = deep_search_naver_internal(
        refined_query=refined.refined_query,
        needs_recency=refined.needs_recency,
        max_results=max_results,
    )

    # 3) ìš”ì•½
    summarized = summarize_naver_results(raw_results)

    # 4) í¬ë§·íŒ…
    output = format_search_output(summarized)

    header = (
        f"[NAVER_SEARCH]\n"
        f"refined_query: {refined.refined_query}\n"
        f"needs_recency: {refined.needs_recency}\n\n"
    )
    return header + output


@tool(parse_docstring=True)
def naver_search(
    question: str,
    max_results: Annotated[int, "Maximum number of sources to use"] = 5,
) -> str:
    """
    High-precision Korean web/news/blog search using Naver OpenAPI with **query refinement** and **page scraping**.

    This tool:
    1) Uses an LLM to refine the original Korean question into a concise Naver-friendly search query.
    2) Calls Naver's `news`, `webkr`, and `blog` search APIs with appropriate sorting (by date if time-sensitive).
    3) Visits each result page with Playwright and extracts the main body text (news article / blog post / etc.).
    4) Summarizes the cleaned content using the project's summarization model and `summarize_webpage_prompt`.
    5) Returns a formatted string similar to the Tavily search tool, including URL and SUMMARY for each source.

    Args:
        question: Original Korean question or topic from the user.
        max_results: Maximum number of documents (URLs) to include in the formatted output.

    Returns:
        A formatted string containing refined query, URLs, and summarized content for each result.
        This string is designed to be consumed by the research agent in the same way as the Tavily-based search tool.
    """
    # âœ… ì ˆëŒ€ asyncio.run(...) ì“°ì§€ ë§ê³ , ê·¸ëƒ¥ sync êµ¬í˜„ í˜¸ì¶œ
    return _naver_search_impl(question, max_results)
